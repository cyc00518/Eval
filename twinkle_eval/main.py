import argparse
import copy
import os
import time
from datetime import datetime
from typing import Any, Dict, List, Optional

import numpy as np

from twinkle_eval.exceptions import ConfigurationError

from .config import load_config
from .dataset import find_all_evaluation_files
from .evaluation_strategies import EvaluationStrategyFactory
from .evaluators import Evaluator
from .logger import log_error, log_info
from .results_exporters import ResultsExporterFactory


def convert_json_to_html(json_file_path: str) -> int:
    """å°‡ JSON çµæœæª”æ¡ˆè½‰æ›ç‚º HTML æ ¼å¼

    Args:
        json_file_path: JSON çµæœæª”æ¡ˆçš„è·¯å¾‘

    Returns:
        int: ç¨‹å¼é€€å‡ºä»£ç¢¼ï¼ˆ0 è¡¨ç¤ºæˆåŠŸï¼Œ1 è¡¨ç¤ºå¤±æ•—ï¼‰
    """
    import json

    try:
        # æª¢æŸ¥è¼¸å…¥æª”æ¡ˆæ˜¯å¦å­˜åœ¨
        if not os.path.exists(json_file_path):
            print(f"âŒ æª”æ¡ˆä¸å­˜åœ¨: {json_file_path}")
            return 1

        # è¼‰å…¥ JSON çµæœ
        with open(json_file_path, "r", encoding="utf-8") as f:
            results = json.load(f)

        # å»ºç«‹ HTML è¼¸å‡ºå™¨
        html_exporter = ResultsExporterFactory.create_exporter("html")

        # ç”¢ç”Ÿè¼¸å‡ºæª”æ¡ˆè·¯å¾‘ï¼ˆèˆ‡è¼¸å…¥æª”æ¡ˆåŒç›®éŒ„ï¼Œä½†å‰¯æª”åç‚º .htmlï¼‰
        output_path = os.path.splitext(json_file_path)[0] + ".html"

        # åŸ·è¡Œè½‰æ›
        exported_file = html_exporter.export(results, output_path)

        print(f"âœ… æˆåŠŸè½‰æ›ç‚º HTML: {exported_file}")
        return 0

    except json.JSONDecodeError as e:
        print(f"âŒ JSON æª”æ¡ˆæ ¼å¼éŒ¯èª¤: {e}")
        return 1
    except Exception as e:
        print(f"âŒ è½‰æ›éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
        return 1


def create_default_config(output_path: str = "config.yaml") -> int:
    """å‰µå»ºé è¨­é…ç½®æª”æ¡ˆ

    Args:
        output_path: è¼¸å‡ºæª”æ¡ˆè·¯å¾‘ï¼Œé è¨­ç‚º config.yaml

    Returns:
        int: ç¨‹å¼é€€å‡ºä»£ç¢¼ï¼ˆ0 è¡¨ç¤ºæˆåŠŸï¼Œ1 è¡¨ç¤ºå¤±æ•—ï¼‰
    """
    import shutil

    try:
        # æª¢æŸ¥æª”æ¡ˆæ˜¯å¦å·²å­˜åœ¨
        if os.path.exists(output_path):
            response = input(f"âš ï¸  æª”æ¡ˆ '{output_path}' å·²å­˜åœ¨ï¼Œæ˜¯å¦è¦†è“‹ï¼Ÿ(y/N): ")
            if response.lower() not in ["y", "yes", "æ˜¯"]:
                print("âŒ å–æ¶ˆå‰µå»ºé…ç½®æª”æ¡ˆ")
                return 1

        # æ‰¾åˆ°ç¯„æœ¬æª”æ¡ˆ
        template_path = os.path.join(os.path.dirname(__file__), "config.template.yaml")

        if not os.path.exists(template_path):
            print(f"âŒ æ‰¾ä¸åˆ°é…ç½®ç¯„æœ¬æª”æ¡ˆ: {template_path}")
            return 1

        # è¤‡è£½ç¯„æœ¬æª”æ¡ˆ
        shutil.copy2(template_path, output_path)

        print(f"âœ… é…ç½®æª”æ¡ˆå·²å‰µå»º: {output_path}")
        print()
        print("ğŸ“ æ¥ä¸‹ä¾†è«‹ç·¨è¼¯é…ç½®æª”æ¡ˆï¼Œè¨­å®šï¼š")
        print("  1. LLM API è¨­å®š (base_url, api_key)")
        print("  2. æ¨¡å‹åç¨± (model.name)")
        print("  3. è³‡æ–™é›†è·¯å¾‘ (evaluation.dataset_paths)")
        print()
        print("ğŸ’¡ ç·¨è¼¯å®Œæˆå¾Œï¼Œä½¿ç”¨ä»¥ä¸‹å‘½ä»¤é–‹å§‹è©•æ¸¬ï¼š")
        print(f"   twinkle-eval --config {output_path}")

        return 0

    except Exception as e:
        print(f"âŒ å‰µå»ºé…ç½®æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return 1


class TwinkleEvalRunner:
    """Twinkle Eval ä¸»è¦åŸ·è¡Œå™¨é¡åˆ¥ - è² è²¬æ§åˆ¶æ•´å€‹è©•æ¸¬æµç¨‹"""

    def __init__(self, config_path: str = "config.yaml"):
        """åˆå§‹åŒ– Twinkle Eval åŸ·è¡Œå™¨

        Args:
            config_path: é…ç½®æª”æ¡ˆè·¯å¾‘ï¼Œé è¨­ç‚º config.yaml
        """
        self.config_path = config_path  # é…ç½®æª”æ¡ˆè·¯å¾‘
        self.config = None  # è¼‰å…¥çš„é…ç½®å­—å…¸
        self.start_time = None  # åŸ·è¡Œé–‹å§‹æ™‚é–“æ¨™è¨˜
        self.start_datetime = None  # åŸ·è¡Œé–‹å§‹çš„ datetime ç‰©ä»¶
        self.results_dir = "results"  # çµæœè¼¸å‡ºç›®éŒ„

    def initialize(self):
        """åˆå§‹åŒ–è©•æ¸¬åŸ·è¡Œå™¨

        è¼‰å…¥é…ç½®ã€è¨­å®šæ™‚é–“æ¨™è¨˜ã€å»ºç«‹çµæœç›®éŒ„

        Raises:
            Exception: åˆå§‹åŒ–éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤
        """
        try:
            self.config = load_config(self.config_path)  # è¼‰å…¥é…ç½®
            self.start_time = datetime.now().strftime("%Y%m%d_%H%M")  # ç”Ÿæˆæ™‚é–“æ¨™è¨˜
            self.start_datetime = datetime.now()  # è¨˜éŒ„é–‹å§‹æ™‚é–“

            os.makedirs(self.results_dir, exist_ok=True)  # å»ºç«‹çµæœç›®éŒ„

            log_info(f"Twinkle Eval åˆå§‹åŒ–å®Œæˆ - {self.start_time}")

        except Exception as e:
            log_error(f"åˆå§‹åŒ–å¤±æ•—: {e}")
            raise

    def _prepare_config_for_saving(self) -> Dict[str, Any]:
        """æº–å‚™ç”¨æ–¼å„²å­˜çš„é…ç½®è³‡æ–™ï¼Œç§»é™¤æ•æ„Ÿè³‡è¨Š

        åœ¨å„²å­˜é…ç½®åˆ°çµæœæª”æ¡ˆå‰ï¼Œéœ€è¦ç§»é™¤ API é‡‘é‘°ç­‰æ•æ„Ÿè³‡è¨Š
        å’Œä¸å¯åºåˆ—åŒ–çš„ç‰©ä»¶å¯¦ä¾‹

        Returns:
            Dict[str, Any]: æ¸…ç†å¾Œçš„é…ç½®å­—å…¸
        """
        if self.config is None:
            raise ConfigurationError("é…ç½®æœªè¼‰å…¥")

        # ç§»é™¤ç‰©ä»¶å¯¦ä¾‹ï¼ˆä¸å¯åºåˆ—åŒ–ï¼‰
        if "llm_instance" in self.config:
            del self.config["llm_instance"]

        save_config = copy.deepcopy(self.config)

        # ç§»é™¤æ•æ„Ÿè³‡è¨Šï¼ˆAPI é‡‘é‘°ï¼‰
        if "llm_api" in save_config and "api_key" in save_config["llm_api"]:
            del save_config["llm_api"]["api_key"]
        if "evaluation_strategy_instance" in save_config:
            del save_config["evaluation_strategy_instance"]

        return save_config

    def _get_dataset_paths(self) -> List[str]:
        """å¾é…ç½®ä¸­å–å¾—è³‡æ–™é›†è·¯å¾‘æ¸…å–®

        æ”¯æ´å–®ä¸€è·¯å¾‘å­—ä¸²æˆ–è·¯å¾‘æ¸…å–®ï¼Œçµ±ä¸€è½‰æ›ç‚ºæ¸…å–®æ ¼å¼

        Returns:
            List[str]: è³‡æ–™é›†è·¯å¾‘æ¸…å–®
        """
        if self.config is None:
            raise ConfigurationError("é…ç½®æœªè¼‰å…¥")

        dataset_paths = self.config["evaluation"]["dataset_paths"]
        if isinstance(dataset_paths, str):
            dataset_paths = [dataset_paths]
        return dataset_paths

    def _resolve_eval_method(self, dataset_path: str) -> str:
        """æ ¹æ“šè³‡æ–™é›†è·¯å¾‘æ±ºå®šè¦ç”¨çš„è©•æ¸¬æ–¹æ³•ï¼ˆå¯ç”± config è¦†å¯«ï¼‰"""
        if self.config is None:
            raise ConfigurationError("é…ç½®æœªè¼‰å…¥")

        eval_cfg = self.config["evaluation"]
        method_map = eval_cfg.get("dataset_method_map", {})
        overrides = eval_cfg.get("dataset_overrides", {})

        dataset_abs = os.path.normpath(os.path.abspath(dataset_path))

        for prefix, settings in overrides.items():
            try:
                prefix_abs = os.path.normpath(os.path.abspath(prefix))
                if dataset_abs.startswith(prefix_abs) and isinstance(settings, dict):
                    method = settings.get("evaluation_method")
                    if method:
                        return method
            except (OSError, TypeError, ValueError) as e:
                settings_info = (
                    f"keys={list(settings.keys())}" if isinstance(settings, dict) else f"type={type(settings).__name__}"
                )
                log_error(
                    f"å¥—ç”¨ dataset_overrides è©•æ¸¬æ–¹æ³•å¤±æ•— (dataset={dataset_path}, override_prefix={prefix}, {settings_info}): {e}"
                )
                continue

        for prefix, method in method_map.items():
            try:
                prefix_abs = os.path.normpath(os.path.abspath(prefix))
                if dataset_abs.startswith(prefix_abs):
                    return method
            except (OSError, TypeError, ValueError) as e:
                log_error(
                    f"å¥—ç”¨ dataset_method_map å¤±æ•— (dataset={dataset_path}, method_map_prefix={prefix}): {e}"
                )
                continue

        return eval_cfg["evaluation_method"]

    def _resolve_system_prompt_enabled(self, dataset_path: str) -> bool:
        """æ±ºå®šæ˜¯å¦å°æ­¤è³‡æ–™é›†ä½¿ç”¨ system promptã€‚"""
        if self.config is None:
            raise ConfigurationError("é…ç½®æœªè¼‰å…¥")

        eval_cfg = self.config["evaluation"]
        overrides = eval_cfg.get("dataset_overrides", {})
        dataset_abs = os.path.normpath(os.path.abspath(dataset_path))

        for prefix, settings in overrides.items():
            try:
                prefix_abs = os.path.normpath(os.path.abspath(prefix))
                if dataset_abs.startswith(prefix_abs) and isinstance(settings, dict):
                    if "system_prompt_enabled" in settings:
                        return bool(settings["system_prompt_enabled"])
            except (OSError, TypeError, ValueError) as e:
                settings_info = (
                    f"keys={list(settings.keys())}" if isinstance(settings, dict) else f"type={type(settings).__name__}"
                )
                log_error(
                    f"å¥—ç”¨ system_prompt_enabled è¦†å¯«å¤±æ•— (dataset={dataset_path}, override_prefix={prefix}, {settings_info}): {e}"
                )
                continue

        return bool(eval_cfg.get("system_prompt_enabled", True))

    def _resolve_dataset_settings(self, dataset_path: str) -> Dict[str, Any]:
        """æ•´åˆè³‡æ–™é›†å°ˆå±¬è¨­å®šï¼ˆæ¨¡å¼ã€promptã€æŠ½æ¨£ã€pass@kã€é‡è¤‡æ¬¡æ•¸ã€shuffleã€æ¨¡å‹è¦†å¯«ï¼‰ã€‚"""
        if self.config is None:
            raise ConfigurationError("é…ç½®æœªè¼‰å…¥")

        eval_cfg = self.config["evaluation"]
        overrides = eval_cfg.get("dataset_overrides", {})
        dataset_abs = os.path.normpath(os.path.abspath(dataset_path))

        settings = {
            "evaluation_method": self._resolve_eval_method(dataset_path),
            "system_prompt_enabled": self._resolve_system_prompt_enabled(dataset_path),
            "samples_per_question": eval_cfg.get("samples_per_question", 1),
            "pass_k": eval_cfg.get("pass_k", 1),
            "repeat_runs": eval_cfg.get("repeat_runs", 1),
            "shuffle_options": eval_cfg.get("shuffle_options", False),
            "model_overrides": {},
        }

        for prefix, cfg in overrides.items():
            try:
                prefix_abs = os.path.normpath(os.path.abspath(prefix))
                if dataset_abs.startswith(prefix_abs) and isinstance(cfg, dict):
                    if "evaluation_method" in cfg:
                        settings["evaluation_method"] = cfg["evaluation_method"]
                    if "system_prompt_enabled" in cfg:
                        settings["system_prompt_enabled"] = cfg["system_prompt_enabled"]
                    if "samples_per_question" in cfg:
                        settings["samples_per_question"] = cfg["samples_per_question"]
                    if "pass_k" in cfg:
                        settings["pass_k"] = cfg["pass_k"]
                    if "repeat_runs" in cfg:
                        settings["repeat_runs"] = cfg["repeat_runs"]
                    if "shuffle_options" in cfg:
                        settings["shuffle_options"] = cfg["shuffle_options"]
                    for mk in ("temperature", "top_p", "max_tokens", "frequency_penalty", "presence_penalty"):
                        if mk in cfg:
                            settings["model_overrides"][mk] = cfg[mk]
            except (OSError, TypeError, ValueError) as e:
                cfg_info = f"keys={list(cfg.keys())}" if isinstance(cfg, dict) else f"type={type(cfg).__name__}"
                log_error(
                    f"å¥—ç”¨ dataset_overrides è¨­å®šå¤±æ•— (dataset={dataset_path}, override_prefix={prefix}, {cfg_info}): {e}"
                )
                continue

        return settings

    def _evaluate_dataset(
        self,
        dataset_path: str,
        evaluator: Evaluator,
        repeat_runs: int,
        dataset_label: str,
        pass_k: int,
    ) -> Dict[str, Any]:
        """è©•æ¸¬å–®ä¸€è³‡æ–™é›†

        å°æŒ‡å®šè³‡æ–™é›†ä¸­çš„æ‰€æœ‰æª”æ¡ˆé€²è¡Œè©•æ¸¬ï¼Œæ”¯æ´å¤šæ¬¡åŸ·è¡Œä¸¦çµ±è¨ˆçµæœ

        Args:
            dataset_path: è³‡æ–™é›†è·¯å¾‘
            evaluator: è©•æ¸¬å™¨å¯¦ä¾‹

        Returns:
            Dict[str, Any]: è³‡æ–™é›†è©•æ¸¬çµæœï¼ŒåŒ…å«æº–ç¢ºç‡çµ±è¨ˆå’Œè©³ç´°çµæœ
        """
        if self.config is None:
            raise ConfigurationError("é…ç½®æœªè¼‰å…¥")

        log_info(f"é–‹å§‹è©•æ¸¬è³‡æ–™é›†: {dataset_path}")

        all_files = find_all_evaluation_files(dataset_path)  # å°‹æ‰¾æ‰€æœ‰è©•æ¸¬æª”æ¡ˆ
        prompt_map = self.config["evaluation"].get("datasets_prompt_map", {})  # è³‡æ–™é›†èªè¨€å°æ‡‰è¡¨
        dataset_lang = prompt_map.get(dataset_path, "zh")  # ç•¶å‰è³‡æ–™é›†çš„èªè¨€ï¼Œé è¨­ç‚ºä¸­æ–‡

        results = []  # å„²å­˜æ‰€æœ‰æª”æ¡ˆçš„è©•æ¸¬çµæœ

        for idx, file_path in enumerate(all_files):
            file_accuracies = []  # ç•¶å‰æª”æ¡ˆçš„æº–ç¢ºç‡çµæœ
            file_pass_ats = []  # ç•¶å‰æª”æ¡ˆçš„ pass@k çµæœ
            file_results = []  # ç•¶å‰æª”æ¡ˆçš„è©³ç´°çµæœ
            file_pass_metric_labels = []

            # å°ç•¶å‰æª”æ¡ˆé€²è¡Œå¤šæ¬¡è©•æ¸¬
            for run in range(repeat_runs):
                try:
                    file_path_result, metrics, result_path = evaluator.evaluate_file(
                        file_path, f"{self.start_time}_run{run}", dataset_lang, dataset_label
                    )
                    file_accuracies.append(metrics.get("accuracy", 0))
                    file_pass_ats.append(metrics.get("pass_at_k", 0))
                    file_pass_metric_labels.append(metrics.get("pass_metric"))
                    file_results.append((file_path_result, metrics, result_path))
                except Exception as e:
                    log_error(f"è©•æ¸¬æª”æ¡ˆ {file_path} å¤±æ•—: {e}")
                    continue

            # ç‚ºç•¶å‰æª”æ¡ˆè¨ˆç®—çµ±è¨ˆæ•¸æ“š
            if file_accuracies:
                mean_accuracy = np.mean(file_accuracies)  # å¹³å‡æº–ç¢ºç‡
                std_accuracy = np.std(file_accuracies) if len(file_accuracies) > 1 else 0  # æ¨™æº–å·®
                mean_pass_at_k = np.mean(file_pass_ats) if file_pass_ats else 0
                pass_metric_label = (
                    file_pass_metric_labels[-1]
                    if file_pass_metric_labels and file_pass_metric_labels[-1]
                    else f"pass@{pass_k}"
                )

                results.append(
                    {
                        "file": file_path,
                        "accuracy_mean": mean_accuracy,
                        "accuracy_std": std_accuracy,
                        "pass_at_k_mean": mean_pass_at_k,
                        "pass_metric": pass_metric_label,
                        "pass_k": pass_k,
                        "individual_runs": {
                            "accuracies": file_accuracies,
                            "pass_at_k": file_pass_ats,
                            "results": [r[2] for r in file_results],
                            "metrics": [r[1] for r in file_results],
                        },
                    }
                )

            # é€²åº¦æŒ‡ç¤ºå™¨
            progress = (idx + 1) / len(all_files) * 100
            print(f"\rå·²åŸ·è¡Œ {progress:.1f}% ({idx + 1}/{len(all_files)}) ", end="")

        print()  # é€²åº¦å®Œæˆå¾Œæ›è¡Œ

        # è¨ˆç®—è³‡æ–™é›†çµ±è¨ˆæ•¸æ“š
        dataset_avg_accuracy = (
            np.mean([r["accuracy_mean"] for r in results]) if results else 0
        )  # è³‡æ–™é›†å¹³å‡æº–ç¢ºç‡
        dataset_avg_std = (
            np.mean([r["accuracy_std"] for r in results]) if results else 0
        )  # è³‡æ–™é›†å¹³å‡æ¨™æº–å·®
        dataset_avg_pass_at_k = np.mean([r["pass_at_k_mean"] for r in results]) if results else 0

        return {
            "results": results,
            "average_accuracy": dataset_avg_accuracy,
            "average_std": dataset_avg_std,
            "average_pass_at_k": dataset_avg_pass_at_k,
            "pass_metric": f"pass@{pass_k}",
            "pass_k": pass_k,
        }

    def run_evaluation(self, export_formats: Optional[List[str]] = None) -> str:
        """åŸ·è¡Œå®Œæ•´çš„è©•æ¸¬æµç¨‹

        é€™æ˜¯ä¸»è¦çš„è©•æ¸¬å…¥å£é»ï¼ŒåŒ…å«ä»¥ä¸‹æ­¥é©Ÿï¼š
        1. å»ºç«‹è©•æ¸¬å™¨
        2. å°æ‰€æœ‰è³‡æ–™é›†é€²è¡Œè©•æ¸¬
        3. çµ±è¨ˆå’Œè¼¸å‡ºçµæœ

        Args:
            export_formats: è¼¸å‡ºæ ¼å¼æ¸…å–®ï¼Œé è¨­ç‚º ["json"]

        Returns:
            str: ä¸»è¦çµæœæª”æ¡ˆè·¯å¾‘
        """
        if self.config is None:
            raise ConfigurationError("é…ç½®æœªè¼‰å…¥")

        if export_formats is None:
            export_formats = ["json"]  # é è¨­è¼¸å‡ºæ ¼å¼

        dataset_paths = self._get_dataset_paths()  # å–å¾—è³‡æ–™é›†è·¯å¾‘
        dataset_results = {}  # å„²å­˜æ‰€æœ‰è³‡æ–™é›†çš„çµæœ

        # å»ºç«‹è©•æ¸¬å™¨ï¼ˆå«ç­–ç•¥å¿«å–ï¼‰
        llm_instance = self.config["llm_instance"]
        default_strategy = self.config["evaluation_strategy_instance"]
        strategy_config = self.config["evaluation"].get("strategy_config", {})
        strategy_cache = {self.config["evaluation"]["evaluation_method"]: default_strategy}

        # é€ä¸€è©•æ¸¬æ¯å€‹è³‡æ–™é›†
        for dataset_path in dataset_paths:
            try:
                ds_settings = self._resolve_dataset_settings(dataset_path)
                eval_method = ds_settings["evaluation_method"]

                if eval_method not in strategy_cache:
                    strategy_cache[eval_method] = EvaluationStrategyFactory.create_strategy(
                        eval_method, strategy_config
                    )

                evaluator = Evaluator(
                    llm_instance,
                    strategy_cache[eval_method],
                    self.config,
                    eval_method,
                    ds_settings["system_prompt_enabled"],
                    ds_settings["samples_per_question"],
                    ds_settings["pass_k"],
                    ds_settings["shuffle_options"],
                    ds_settings.get("model_overrides", {}),
                )
                dataset_label = os.path.basename(os.path.normpath(dataset_path))
                dataset_result = self._evaluate_dataset(
                    dataset_path,
                    evaluator,
                    repeat_runs=ds_settings["repeat_runs"],
                    dataset_label=dataset_label,
                    pass_k=ds_settings["pass_k"],
                )
                dataset_result["evaluation_method"] = eval_method
                dataset_result["pass_metric"] = f"pass@{ds_settings['pass_k']}"
                dataset_result["pass_k"] = ds_settings["pass_k"]
                dataset_results[dataset_path] = dataset_result

                message = (
                    f"è³‡æ–™é›† {dataset_path} è©•æ¸¬å®Œæˆï¼ˆæ¨¡å¼: {eval_method}ï¼‰ï¼Œ"
                    f"å¹³å‡æ­£ç¢ºç‡: {dataset_result['average_accuracy']:.2%} "
                    f"(Â±{dataset_result['average_std']:.2%})"
                )
                print(message)
                log_info(message)

            except Exception as e:
                log_error(f"è³‡æ–™é›† {dataset_path} è©•æ¸¬å¤±æ•—: {e}")
                continue

        # æº–å‚™æœ€çµ‚çµæœ
        current_duration = (
            (datetime.now() - self.start_datetime).total_seconds() if self.start_datetime else 0
        )  # è¨ˆç®—åŸ·è¡Œæ™‚é–“
        final_results = {
            "timestamp": self.start_time,  # åŸ·è¡Œæ™‚é–“æ¨™è¨˜
            "config": self._prepare_config_for_saving(),  # æ¸…ç†å¾Œçš„é…ç½®
            "dataset_results": dataset_results,  # æ‰€æœ‰è³‡æ–™é›†çµæœ
            "duration_seconds": current_duration,  # åŸ·è¡Œæ™‚é–“ï¼ˆç§’ï¼‰
        }

        # ä»¥å¤šç¨®æ ¼å¼è¼¸å‡ºçµæœ
        base_output_path = os.path.join(self.results_dir, f"results_{self.start_time}")
        exported_files = ResultsExporterFactory.export_results(
            final_results, base_output_path, export_formats, self.config
        )

        # Google æœå‹™æ•´åˆ
        self._handle_google_services(final_results, export_formats)

        log_info(f"è©•æ¸¬å®Œæˆï¼Œçµæœå·²åŒ¯å‡ºè‡³: {', '.join(exported_files)}")
        return exported_files[0] if exported_files else ""

    def _handle_google_services(self, results: Dict[str, Any], export_formats: List[str]):
        """è™•ç† Google æœå‹™æ•´åˆ

        Args:
            results: è©•æ¸¬çµæœå­—å…¸
            export_formats: åŒ¯å‡ºæ ¼å¼åˆ—è¡¨
        """
        google_services_config = self.config.get("google_services")
        if not google_services_config:
            return

        # è™•ç† Google Drive æª”æ¡ˆä¸Šå‚³ï¼ˆæœ€æ–°çš„ log å’Œ resultsï¼‰
        google_drive_config = google_services_config.get("google_drive", {})
        if google_drive_config.get("enabled", False):
            try:
                from .google_services import GoogleDriveUploader

                uploader = GoogleDriveUploader(google_drive_config)
                upload_info = uploader.upload_latest_files(self.start_time, "logs", "results")

                if upload_info.get("uploaded_files"):
                    log_info(
                        f"æˆåŠŸå»ºç«‹è³‡æ–™å¤¾: {upload_info['folder_name']} ({upload_info['folder_id']})"
                    )
                    log_info(f"æˆåŠŸä¸Šå‚³ {len(upload_info['uploaded_files'])} å€‹æª”æ¡ˆåˆ° Google Drive")

                    for file_info in upload_info["uploaded_files"]:
                        log_info(f"  - {file_info['type']}: {file_info['file_name']}")
            except Exception as e:
                log_error(f"Google Drive æª”æ¡ˆä¸Šå‚³å¤±æ•—: {e}")

        # è™•ç† Google Sheets çµæœåŒ¯å‡º
        google_sheets_config = google_services_config.get("google_sheets", {})
        if google_sheets_config.get("enabled", False):
            try:
                # æª¢æŸ¥æ˜¯å¦å·²ç¶“åœ¨ export_formats ä¸­æŒ‡å®š google_sheets
                if "google_sheets" not in export_formats:
                    # å¦‚æœç”¨æˆ¶æ²’æœ‰æ˜ç¢ºæŒ‡å®šï¼Œæˆ‘å€‘è‡ªå‹•åŸ·è¡Œ Google Sheets åŒ¯å‡º
                    sheets_exporter = ResultsExporterFactory.create_exporter(
                        "google_sheets", google_sheets_config
                    )
                    sheets_url = sheets_exporter.export(results, "google_sheets_export")
                    log_info(f"çµæœå·²è‡ªå‹•åŒ¯å‡ºåˆ° Google Sheets: {sheets_url}")
            except Exception as e:
                log_error(f"Google Sheets çµæœåŒ¯å‡ºå¤±æ•—: {e}")


def create_cli_parser() -> argparse.ArgumentParser:
    """å»ºç«‹å‘½ä»¤åˆ—ä»‹é¢è§£æå™¨

    å®šç¾©æ‰€æœ‰å‘½ä»¤åˆ—åƒæ•¸å’Œé¸é …ï¼Œæ”¯æ´å¤šç¨®è©•æ¸¬å’ŒæŸ¥è©¢åŠŸèƒ½

    Returns:
        argparse.ArgumentParser: é…ç½®å®Œæˆçš„å‘½ä»¤åˆ—è§£æå™¨
    """
    parser = argparse.ArgumentParser(
        description="ğŸŒŸ Twinkle Eval - AI æ¨¡å‹è©•æ¸¬å·¥å…·",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¯„ä¾‹:
  twinkle-eval                          # ä½¿ç”¨é è¨­é…ç½®åŸ·è¡Œ
  twinkle-eval --config custom.yaml    # ä½¿ç”¨è‡ªå®šç¾©é…ç½®æª”
  twinkle-eval --export json csv html google_sheets  # è¼¸å‡ºç‚ºå¤šç¨®æ ¼å¼
  twinkle-eval --list-llms             # åˆ—å‡ºå¯ç”¨çš„ LLM é¡å‹
  twinkle-eval --list-strategies       # åˆ—å‡ºå¯ç”¨çš„è©•æ¸¬ç­–ç•¥

çµæœæ ¼å¼è½‰æ›:
  twinkle-eval --convert-to-html results_20240101_1200.json  # å°‡ JSON çµæœè½‰æ›ç‚º HTML

æ•ˆèƒ½åŸºæº–æ¸¬è©¦:
  twinkle-eval --benchmark                           # åŸ·è¡Œé è¨­çš„åŸºæº–æ¸¬è©¦
  twinkle-eval --benchmark --benchmark-requests 50  # åŸ·è¡Œ 50 å€‹è«‹æ±‚çš„æ¸¬è©¦
  twinkle-eval --benchmark --benchmark-concurrency 5 --benchmark-rate 2  # 5 ä¸¦ç™¼ï¼Œ2 è«‹æ±‚/ç§’

HuggingFace è³‡æ–™é›†ä¸‹è¼‰:
  twinkle-eval --download-dataset cais/mmlu          # ä¸‹è¼‰ MMLU æ‰€æœ‰å­é›†
  twinkle-eval --download-dataset cais/mmlu --dataset-subset anatomy  # ä¸‹è¼‰ç‰¹å®šå­é›†
  twinkle-eval --dataset-info cais/mmlu             # æŸ¥çœ‹è³‡æ–™é›†è³‡è¨Š
        """,
    )

    parser.add_argument(
        "--config", "-c", default="config.yaml", help="é…ç½®æª”æ¡ˆè·¯å¾‘ (é è¨­: config.yaml)"
    )

    parser.add_argument(
        "--export",
        "-e",
        nargs="+",
        default=["json"],
        choices=ResultsExporterFactory.get_available_types(),
        help="è¼¸å‡ºæ ¼å¼ (é è¨­: json)",
    )

    parser.add_argument("--list-llms", action="store_true", help="åˆ—å‡ºå¯ç”¨çš„ LLM é¡å‹")

    parser.add_argument("--list-strategies", action="store_true", help="åˆ—å‡ºå¯ç”¨çš„è©•æ¸¬ç­–ç•¥")

    parser.add_argument("--list-exporters", action="store_true", help="åˆ—å‡ºå¯ç”¨çš„è¼¸å‡ºæ ¼å¼")

    parser.add_argument("--version", action="store_true", help="é¡¯ç¤ºç‰ˆæœ¬è³‡è¨Š")

    parser.add_argument("--init", action="store_true", help="å‰µå»ºé è¨­é…ç½®æª”æ¡ˆ")

    # HuggingFace è³‡æ–™é›†ä¸‹è¼‰ç›¸é—œå‘½ä»¤
    parser.add_argument(
        "--download-dataset",
        metavar="DATASET_NAME",
        help="å¾ HuggingFace Hub ä¸‹è¼‰è³‡æ–™é›† (ä¾‹å¦‚: cais/mmlu)",
    )

    parser.add_argument(
        "--dataset-subset",
        metavar="SUBSET",
        help="æŒ‡å®šè³‡æ–™é›†å­é›†åç¨± (èˆ‡ --download-dataset ä¸€èµ·ä½¿ç”¨)",
    )

    parser.add_argument(
        "--dataset-split",
        metavar="SPLIT",
        default="test",
        help="æŒ‡å®šè³‡æ–™é›†åˆ†å‰² (é è¨­: test)",
    )

    parser.add_argument(
        "--output-dir",
        metavar="DIR",
        default="datasets",
        help="è³‡æ–™é›†ä¸‹è¼‰è¼¸å‡ºç›®éŒ„ (é è¨­: datasets)",
    )

    parser.add_argument(
        "--dataset-info",
        metavar="DATASET_NAME",
        help="ç²å– HuggingFace è³‡æ–™é›†è³‡è¨Š",
    )

    parser.add_argument(
        "--convert-to-html",
        metavar="JSON_FILE",
        help="å°‡ JSON çµæœæª”æ¡ˆè½‰æ›ç‚º HTML æ ¼å¼",
    )

    # Benchmark ç›¸é—œå‘½ä»¤
    parser.add_argument(
        "--benchmark",
        action="store_true",
        help="åŸ·è¡Œ LLM æ•ˆèƒ½åŸºæº–æ¸¬è©¦",
    )

    parser.add_argument(
        "--benchmark-prompt",
        metavar="PROMPT",
        default="è«‹ç”¨ç¹é«”ä¸­æ–‡å›ç­”ï¼šå°ç£çš„é¦–éƒ½æ˜¯å“ªè£¡ï¼Ÿ",
        help="åŸºæº–æ¸¬è©¦ä½¿ç”¨çš„æç¤ºæ–‡å­— (é è¨­: è«‹ç”¨ç¹é«”ä¸­æ–‡å›ç­”ï¼šå°ç£çš„é¦–éƒ½æ˜¯å“ªè£¡ï¼Ÿ)",
    )

    parser.add_argument(
        "--benchmark-requests",
        type=int,
        default=100,
        help="åŸºæº–æ¸¬è©¦çš„ç¸½è«‹æ±‚æ•¸ (é è¨­: 100)",
    )

    parser.add_argument(
        "--benchmark-concurrency",
        type=int,
        default=10,
        help="åŸºæº–æ¸¬è©¦çš„ä¸¦ç™¼è«‹æ±‚æ•¸ (é è¨­: 10)",
    )

    parser.add_argument(
        "--benchmark-rate",
        type=float,
        help="åŸºæº–æ¸¬è©¦çš„è«‹æ±‚é€Ÿç‡ (è«‹æ±‚/ç§’ï¼Œä¸æŒ‡å®šå‰‡å…¨é€Ÿç™¼é€)",
    )

    parser.add_argument(
        "--benchmark-duration",
        type=float,
        help="åŸºæº–æ¸¬è©¦çš„æœ€å¤§åŸ·è¡Œæ™‚é–“ (ç§’ï¼Œä¸æŒ‡å®šå‰‡åŸ·è¡Œå®Œæ‰€æœ‰è«‹æ±‚)",
    )

    return parser


def main() -> int:
    """ä¸»ç¨‹å¼å…¥å£é»

    è™•ç†å‘½ä»¤åˆ—åƒæ•¸ä¸¦åŸ·è¡Œç›¸æ‡‰çš„åŠŸèƒ½ï¼ŒåŒ…æ‹¬æŸ¥è©¢åŠŸèƒ½å’Œä¸»è¦è©•æ¸¬æµç¨‹

    Returns:
        int: ç¨‹å¼é€€å‡ºä»£ç¢¼ï¼ˆ0 è¡¨ç¤ºæˆåŠŸï¼Œ1 è¡¨ç¤ºå¤±æ•—ï¼‰
    """
    parser = create_cli_parser()
    args = parser.parse_args()

    # è™•ç†æŸ¥è©¢å‘½ä»¤
    if args.list_llms:
        from .models import LLMFactory

        print("å¯ç”¨çš„ LLM é¡å‹:")
        for llm_type in LLMFactory.get_available_types():
            print(f"  - {llm_type}")
        return 0

    if args.list_strategies:
        from .evaluation_strategies import EvaluationStrategyFactory

        print("å¯ç”¨çš„è©•æ¸¬ç­–ç•¥:")
        for strategy in EvaluationStrategyFactory.get_available_types():
            print(f"  - {strategy}")
        return 0

    if args.list_exporters:
        print("å¯ç”¨çš„è¼¸å‡ºæ ¼å¼:")
        for exporter in ResultsExporterFactory.get_available_types():
            print(f"  - {exporter}")
        return 0

    if args.version:
        from . import get_info

        info = get_info()
        print(f"ğŸŒŸ {info['name']} v{info['version']}")
        print(f"ä½œè€…: {info['author']}")
        print(f"æˆæ¬Š: {info['license']}")
        print(f"ç¶²å€: {info['url']}")
        return 0

    if args.init:
        return create_default_config()

    # HuggingFace è³‡æ–™é›†ç›¸é—œå‘½ä»¤
    if args.download_dataset:
        try:
            from .dataset import download_huggingface_dataset

            download_huggingface_dataset(
                dataset_name=args.download_dataset,
                subset=args.dataset_subset,
                split=args.dataset_split,
                output_dir=args.output_dir,
            )
            print(f"âœ… è³‡æ–™é›†ä¸‹è¼‰å®Œæˆï¼Œå·²å¿«å–åˆ° HuggingFace ç›®éŒ„")
            return 0
        except Exception as e:
            print(f"âŒ ä¸‹è¼‰è³‡æ–™é›†å¤±æ•—: {e}")
            return 1

    if args.dataset_info:
        try:
            from .dataset import list_huggingface_dataset_info

            info = list_huggingface_dataset_info(
                dataset_name=args.dataset_info, subset=args.dataset_subset
            )
            print(f"ğŸ“Š è³‡æ–™é›†è³‡è¨Š: {info['dataset_name']}")
            print(f"å¯ç”¨é…ç½®: {', '.join(info['configs'])}")
            for config, splits in info["splits"].items():
                print(f"  {config}: {', '.join(splits)}")
            return 0
        except Exception as e:
            print(f"âŒ ç²å–è³‡æ–™é›†è³‡è¨Šå¤±æ•—: {e}")
            return 1

    # JSON è½‰ HTML å‘½ä»¤
    if args.convert_to_html:
        try:
            return convert_json_to_html(args.convert_to_html)
        except Exception as e:
            print(f"âŒ è½‰æ›å¤±æ•—: {e}")
            return 1

    # Benchmark å‘½ä»¤
    if args.benchmark:
        try:
            from .benchmark import BenchmarkRunner, print_benchmark_summary, save_benchmark_results
            from .config import load_config

            config = load_config(args.config)
            runner = BenchmarkRunner(config)

            print(f"ğŸš€ é–‹å§‹åŸ·è¡Œ LLM æ•ˆèƒ½åŸºæº–æ¸¬è©¦")
            print(f"   æç¤ºæ–‡å­—: {args.benchmark_prompt}")
            print(f"   è«‹æ±‚æ•¸é‡: {args.benchmark_requests}")
            print(f"   ä¸¦ç™¼æ•¸é‡: {args.benchmark_concurrency}")
            if args.benchmark_rate:
                print(f"   è«‹æ±‚é€Ÿç‡: {args.benchmark_rate} è«‹æ±‚/ç§’")
            if args.benchmark_duration:
                print(f"   æœ€å¤§æ™‚é–“: {args.benchmark_duration} ç§’")
            print("-" * 60)

            metrics = runner.run_benchmark(
                prompt=args.benchmark_prompt,
                num_requests=args.benchmark_requests,
                concurrent_requests=args.benchmark_concurrency,
                request_rate=args.benchmark_rate,
                duration=args.benchmark_duration,
            )

            # é¡¯ç¤ºçµæœæ‘˜è¦
            print_benchmark_summary(metrics)

            # å„²å­˜çµæœ
            timestamp = time.strftime("%Y%m%d_%H%M%S")
            output_path = f"benchmark_results_{timestamp}.json"
            if "llm_instance" in config:
                del config["llm_instance"]
            if "evaluation_strategy_instance" in config:
                del config["evaluation_strategy_instance"]
            save_benchmark_results(metrics, output_path, config)

            return 0

        except Exception as e:
            print(f"âŒ åŸºæº–æ¸¬è©¦å¤±æ•—: {e}")
            log_error(f"åŸºæº–æ¸¬è©¦åŸ·è¡ŒéŒ¯èª¤: {e}")
            return 1

    # åŸ·è¡Œè©•æ¸¬
    try:
        runner = TwinkleEvalRunner(args.config)
        runner.initialize()
        runner.run_evaluation(args.export)
    except Exception as e:
        log_error(f"åŸ·è¡Œå¤±æ•—: {e}")
        return 1

    return 0


if __name__ == "__main__":
    exit(main())
